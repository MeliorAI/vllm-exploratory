{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to explore the use of open source models such as \"facebook/opt-125m\" and \"neuralmagic/Llama-2-7b-chat-quantized.w8a8\", these models are relatively small in size and can be used from my g4dn.2xlarge instance.\n",
    "In addition I compare the output from the open source model to openai.\n",
    "The opensource model is loaded with vllm serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch -q\n",
    "!pip install langchain -q\n",
    "!pip install -U langchain-community -q\n",
    "!pip install python-dotenv openai -q\n",
    "!pip3 install pysqlite3-binary -q\n",
    "!pip install -U sentence-transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "def download_file_from_s3(bucket_name, s3_file_key):\n",
    "    # download files to local environment\n",
    "    # Create an S3 client\n",
    "    s3 = boto3.client('s3')\n",
    "    local_file_path = s3_file_key.split('/')[-1]\n",
    "    # Download the file from S3\n",
    "    s3.download_file(bucket_name, s3_file_key, local_file_path)\n",
    "    print(f\"File {s3_file_key} downloaded from {bucket_name} to {local_file_path}\")\n",
    "\n",
    "def delete_file(file_path):\n",
    "    os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "_RE_COMBINE_WHITESPACE = re.compile(r\"[ ]+\", re.ASCII)\n",
    "_RE_SHORT_LINES = re.compile(\"^.{1,3}\\n\", re.MULTILINE)\n",
    "_RE_MULTILINE_BREAKS = re.compile(\"\\n+\", re.MULTILINE)\n",
    "_RE_PAGE_CHAR = \"\\x0c\"\n",
    "_RE_LATIN_WHITESPACE_CHAR = re.compile(\"\\xa0\", re.ASCII)\n",
    "\n",
    "\n",
    "# @markdown  - **clean_text** - clean text spaces,non-printable and line breaks\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text from several white-space and line-breaks\"\"\"\n",
    "    # remove several line breaks\n",
    "    text = _RE_LATIN_WHITESPACE_CHAR.sub(\" \", text)\n",
    "    # remove several white spaces\n",
    "    text = _RE_COMBINE_WHITESPACE.sub(\" \", text).strip()\n",
    "    # remove very short lines\n",
    "    text = _RE_SHORT_LINES.sub(\"\\n\", text)\n",
    "    # remove several line breaks\n",
    "    text = _RE_MULTILINE_BREAKS.sub(\"\\n\", text)\n",
    "    # remove unknown characters or non printable\n",
    "    text = \"\".join([x for x in text if x in string.printable])\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.base import Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "\n",
    "class SentenceTransformerEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, documents: List[str]) -> List[List[float]]:\n",
    "        return [self.model.encode(d).tolist() for d in documents]\n",
    "\n",
    "    def embed_query(self, query: str) -> List[float]:\n",
    "        return self.model.encode([query])[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the test data I will use some (parsed) files from here s3://contract-intelligence-data/client-data/AAA/NY State Insurance/06-FRM-AR1/ \n",
    "# these are files of good quality\n",
    "\n",
    "download_file_from_s3(\"contract-intelligence-data\", \"client-data/AAA/NY State Insurance/06-FRM-AR1/FRM-AR117-21-1230-2624_2024_163320/FRM-AR117-21-1230-2624_2024_163320.json\")\n",
    "download_file_from_s3(\"contract-intelligence-data\", \"client-data/AAA/NY State Insurance/06-FRM-AR1/FRM-AR117-21-1230-2638_2024_162334/FRM-AR117-21-1230-2638_2024_162334.json\")\n",
    "download_file_from_s3(\"contract-intelligence-data\", \"client-data/AAA/NY State Insurance/06-FRM-AR1/FRM-AR117-22-1252-6330_2024_16400/FRM-AR117-22-1252-6330_2024_16400.json\")\n",
    "\n",
    "download_file_from_s3(\"contract-intelligence-data\",\"client-data/AAA/NY State Insurance/04-RPT-INIT/17-22-1250-8464/17-22-1250-8464.json\")\n",
    "download_file_from_s3(\"contract-intelligence-data\",\"client-data/dragados/ol-elevated-guideway-and-stations-dmca-redacted-version.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "def read_files(docs_dir: str):\n",
    "    files = glob.glob(os.path.join(docs_dir,\"*.json\"), recursive=True)\n",
    "    print(f\"Total number of docs: {len(files)}\")\n",
    "    return files\n",
    "\n",
    "def compose_dataset(docs_dir: str):\n",
    "    files = read_files(docs_dir)\n",
    "    print(files)\n",
    "    # Read & Load the Dataset\n",
    "    dataset = []\n",
    "    for file in tqdm(files):\n",
    "        # data in json format after ocr\n",
    "        with open(file) as f:\n",
    "            pdoc = json.load(f)\n",
    "        dataset.append(pdoc)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = compose_dataset(\".\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import CohereEmbeddings, OpenAIEmbeddings\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rag part, based on the one in LINT api\n",
    "\n",
    "DEFAULT_CHUNK_SIZE = 3500  #1400 (had to reduce to fit into the facebook/opt-125m model)\n",
    "DEFAULT_CHUNK_OVERLAP = 500\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"#I will still use openai for embeddings\n",
    "# next step can also try and replace the embeddings for opensource ones\n",
    "LLM_MODEL_OPENAI = \"gpt-3.5-turbo\"\n",
    "vector_db_path = './chroma_db'\n",
    "\n",
    "SENTENCE_TRANSFORMER_MODEL = \"multi-qa-mpnet-base-cos-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv, dotenv_values\n",
    "import openai\n",
    "path_to_keys = 'keys.env'\n",
    "temp = dotenv_values(path_to_keys)\n",
    "openai_api_key = temp[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets put the data to chroma db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install hnswlib==0.7.0 -q\n",
    "# !pip install chroma-hnswlib==0.7.3 -q\n",
    "# !pip uninstall hnswlib chroma-hnswlib -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install chromadb==0.5 tiktoken -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "def put_in_Chroma(doc_pages, doc_name, embedding_type=\"openai\"):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=DEFAULT_CHUNK_SIZE, chunk_overlap=DEFAULT_CHUNK_OVERLAP)\n",
    "    doc = [\n",
    "                Document(page_content=clean_text(page), metadata={\"page\": i, \"doc_name\": doc_name})\n",
    "                for i, page in enumerate(doc_pages)\n",
    "            ]\n",
    "    chunks = text_splitter.split_documents(doc)\n",
    "    print('chunks: ', len(chunks))\n",
    "    # Retrieve embedding function from code env resources\n",
    "    \n",
    "    if embedding_type == \"openai\":\n",
    "        print(\"Using OpenAI embeddings\")\n",
    "        embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL, openai_api_key=openai_api_key)\n",
    "    else:\n",
    "        print(\"Using Sentence Transformer embeddings\")\n",
    "        embeddings = SentenceTransformerEmbeddings(SENTENCE_TRANSFORMER_MODEL)\n",
    "\n",
    "    # Index the vector database by embedding then inserting document chunks\n",
    "    db = Chroma.from_documents(chunks,\n",
    "                            embedding=embeddings,\n",
    "                            ids=[str(i) for i in range(len(chunks))],\n",
    "                            persist_directory=vector_db_path)\n",
    "\n",
    "    # Save vector database as persistent files in the output folder\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = 'FRM-AR117-21-1230-2624_2024_163320'\n",
    "# file_name = 'FRM-AR117-21-1230-2638_2024_162334'\n",
    "file_name = 'FRM-AR117-22-1252-6330_2024_16400'\n",
    "file_name = '17-22-1250-8464'\n",
    "file_name = 'ol-elevated-guideway-and-stations-dmca-redacted-version'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in dataset:\n",
    "    if i['name'] == file_name:\n",
    "        doc_pages = i['text']\n",
    "        break\n",
    "print('pages: ', len(doc_pages))\n",
    "db = put_in_Chroma(doc_pages, doc_name=file_name, embedding_type='transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt_llm():\n",
    "    chat_params = {\n",
    "        \"model\": \"gpt-3.5-turbo\", # Bigger context window\n",
    "        \"openai_api_key\": openai_api_key,\n",
    "        \"temperature\": 0.000001, \n",
    "    }\n",
    "    llm = ChatOpenAI(**chat_params)\n",
    "    return llm\n",
    "\n",
    "def qa_retriever_openai(query, vector_db_path, file_id, k=4, embeddings_type=\"openai\"):\n",
    "    if embeddings_type == \"openai\":\n",
    "        print(\"Using OpenAI Embeddings\")\n",
    "        embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL, openai_api_key=openai_api_key)\n",
    "    else:\n",
    "        print(\"Using Sentence Transformer Embeddings\")\n",
    "        embeddings = SentenceTransformerEmbeddings(SENTENCE_TRANSFORMER_MODEL)\n",
    "    vectordb = Chroma(persist_directory=vector_db_path, embedding_function=embeddings)\n",
    "\n",
    "    retriever = vectordb.as_retriever(search_kwargs={\"k\": k, \"filter\": {\"doc_name\": file_id}})\n",
    "\n",
    "    qa = RetrievalQA.from_chain_type(llm=get_gpt_llm(), chain_type=\"stuff\", \n",
    "                                    retriever=retriever, return_source_documents=True)\n",
    "    res = qa({\"query\": query, \"k\": k})\n",
    "    return res, retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who are the parties?\"\n",
    "question = \"When is this agreement entered into?\"\n",
    "\n",
    "# question = \"What type of form is that?\"\n",
    "\n",
    "answer, retriever = \\\n",
    "    qa_retriever_openai(question, vector_db_path=\"/home/ubuntu/yulia/vllm-exploratory/llm/xplore/chroma_db\", \\\n",
    "    file_id=file_name, k=4, embeddings_type=\"transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Openai answer: ', answer['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ran in terminal: `vllm serve neuralmagic/Llama-2-7b-chat-quantized.w8a8 --chat-template templates/template_chatml.jinja`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_server_url = \"http://localhost:8000/v1\"\n",
    "\n",
    "# MODEL = \"facebook/opt-125m\"\n",
    "# MODEL = \"neuralmagic/Llama-2-7b-chat-quantized.w8a8\"\n",
    "MODEL = \"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8\"\n",
    "    \n",
    "llm = ChatOpenAI(\n",
    "    model=MODEL,\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    openai_api_base=inference_server_url,\n",
    "    max_tokens=100,\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_retriever_llama(query, vector_db_path, file_id, k=4):\n",
    "    embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL, openai_api_key=openai_api_key)\n",
    "    vectordb = Chroma(persist_directory=vector_db_path, embedding_function=embeddings)\n",
    "\n",
    "    retriever = vectordb.as_retriever(search_kwargs={\"k\": k, \"filter\": {\"doc_name\": file_id}})\n",
    "\n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", \n",
    "                                    retriever=retriever, return_source_documents=True)\n",
    "    res = qa({\"query\": query})\n",
    "    return res, retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "question = \"Who are the parties?\"\n",
    "question = \"When is this agreement entered into?\"\n",
    "\n",
    "answer_llama, retriever = qa_retriever_llama(question, vector_db_path=\"/home/ubuntu/yulia/vllm-exploratory/llm/xplore/chroma_db\", file_id=file_name, k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer_llama)\n",
    "print(answer_llama['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '\\n--\\n'.join([i.page_content for i in answer['source_documents']])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=MODEL,\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    openai_api_base=inference_server_url,\n",
    "    max_tokens=200,\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are an AI assistant, use the following text to provide answer if you don't know, say you don't know\n",
    "        Context: {context}\n",
    "        Question: {question}\n",
    "        Be concise and short in your response.\n",
    "\"\"\"\n",
    "\n",
    "# context = text\n",
    "question = \"Who are the parties?\"\n",
    "# question = \"Where did the accident occur?\"\n",
    "# question = \"What is the date of the accident?\"\n",
    "# question = \"Was the denial of claim based on late notice to the carrier?\"\n",
    "# question = \"Who is the insurer?\"\n",
    "# question = \"What type of form is that?\"\n",
    "\n",
    "# file_name = 'FRM-AR117-22-1252-6330_2024_16400'\n",
    "\n",
    "vector_db_path = \"/home/ubuntu/yulia/vllm-exploratory/llm/xplore/chroma_db\"\n",
    "my_prompt = PromptTemplate(template=prompt, input_variables=[\"context\", \"question\"])\n",
    "embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL, openai_api_key=openai_api_key)\n",
    "vectordb = Chroma(persist_directory=vector_db_path, embedding_function=embeddings)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4, \"filter\": {\"doc_name\": file_name}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                chain_type=\"stuff\", \n",
    "                                retriever=retriever, \n",
    "                                return_source_documents=True,\n",
    "                                chain_type_kwargs={\"prompt\": my_prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "llama_answer = qa.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llama_answer)\n",
    "print(llama_answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_prompt = PromptTemplate(template=prompt, input_variables=[\"context\", \"question\"])\n",
    "embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL, openai_api_key=openai_api_key)\n",
    "vectordb = Chroma(persist_directory=vector_db_path, embedding_function=embeddings)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4, \"filter\": {\"doc_name\": file_name}})\n",
    "qa = RetrievalQA.from_chain_type(llm=get_gpt_llm(), \n",
    "                                chain_type=\"stuff\", \n",
    "                                retriever=retriever, \n",
    "                                return_source_documents=True,\n",
    "                                chain_type_kwargs={\"prompt\": my_prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "openai_answer = qa.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(openai_answer)\n",
    "print(openai_answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db._collection.get(include=[\"metadatas\",\"documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8\"\n",
    "number_gpus = 1\n",
    "max_model_len = 8192\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=256)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "llm = LLM(model=model_id, tensor_parallel_size=number_gpus, max_model_len=max_model_len, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"write a poem about waterlilies\"},\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
