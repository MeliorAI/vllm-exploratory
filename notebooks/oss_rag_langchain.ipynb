{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî® Setup\n",
    "\n",
    "‚ö† In order to run this notebook you need two separate services:\n",
    "\n",
    "- ChromaDB: `chroma run --path ./data/chromadb --port 7000`\n",
    "- vLLM:     `vllm serve <your-model> --chat-template <your-template>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/code/LLM-exploratory\n",
      "/home/ubuntu/code/LLM-exploratory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/code/LLM-exploratory/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "LOCAL_OPENAI_API_URI = \"http://localhost:8000/v1\"\n",
    "LOCAL_CHROMA_URI = \"localhost:7000\"\n",
    "OPENAI_API_KEY = \"EMPTY\"\n",
    "DATA_PATH = \"./data\"\n",
    "DOCS_PATH = Path(DATA_PATH) / \"processed_docs\"\n",
    "\n",
    "# Change to the root of the repo (RUN ONLY ONCE!)\n",
    "%cd ..\n",
    "print(Path.cwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîå OpenAI-like API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      " - neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Check what model is available from the API\n",
    "client = OpenAI(api_key=OPENAI_API_KEY, base_url=LOCAL_OPENAI_API_URI)\n",
    "available_llms = client.models.list().data\n",
    "print(f\"Available models:\")\n",
    "print(\" - \" + \" - \".join(llm.id for llm in available_llms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat response: Large Language Models (LLMs) have several impressive features, but some of the best things about them include:\n",
      "\n",
      "1. **Multilingual Support**: LLMs can understand and generate text in multiple languages, making them a powerful tool for global communication and translation.\n",
      "2. **Contextual Understanding**: LLMs can comprehend the context of a conversation or text, allowing them to provide more accurate and relevant responses.\n",
      "3. **Creative Writing**: LLMs can generate creative content, such as stories, poems, and dialogues, making them a valuable tool for writers and content creators.\n",
      "4. **Conversational Dialogue**: LLMs can engage in natural-sounding conversations, making them a great tool for customer service, chatbots, and language learning.\n",
      "5. **Knowledge Retrieval**: LLMs can quickly retrieve and provide information on a wide range of topics, making them a valuable resource for research and learning.\n",
      "6. **Sentiment Analysis**: LLMs can analyze text to determine the sentiment or emotional tone, making them a useful tool for sentiment analysis and opinion mining.\n",
      "7. **Language Generation**: LLMs can generate text based on a prompt or input, making them a powerful tool for language translation, text summarization, and content creation.\n",
      "8. **Improved Accuracy**: LLMs can learn from large datasets and improve their accuracy over time, making them a valuable tool for applications where accuracy is crucial.\n",
      "9. **Scalability**: LLMs can be\n"
     ]
    }
   ],
   "source": [
    "chat_response = client.chat.completions.create(\n",
    "    model=available_llms[0].id,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the best thing about LLMs?\"},\n",
    "    ],\n",
    ")\n",
    "print(\"Chat response:\", chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùì DocQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiliazing DocQAClient for OpenAI-like API. LLM='neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/code/LLM-exploratory/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/ubuntu/code/LLM-exploratory/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/ubuntu/code/LLM-exploratory/llm/xplore/rag/qa.py:181: LangChainDeprecationWarning: The function `create_structured_output_chain` was deprecated in LangChain 0.1.1 and will be removed in 1.0. Use :meth:`~ChatOpenAI.with_structured_output` instead.\n",
      "  chain = create_structured_output_chain(\n",
      "/home/ubuntu/code/LLM-exploratory/llm/xplore/rag/qa.py:186: LangChainDeprecationWarning: This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/v0.2/docs/versions/migrating_chains/stuff_docs_chain/\n",
      "  final_qa_chain_pydantic = StuffDocumentsChain(\n",
      "/home/ubuntu/code/LLM-exploratory/llm/xplore/rag/qa.py:192: LangChainDeprecationWarning: This class is deprecated. Use the `create_retrieval_chain` constructor instead. See migration guide here: https://python.langchain.com/v0.2/docs/versions/migrating_chains/retrieval_qa/\n",
      "  \"retriever\": RetrievalQA(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "from llm.xplore.rag.qa import DocQAClient\n",
    "\n",
    "llm_model_id = available_llms[0].id\n",
    "\n",
    "# Init the QA Client\n",
    "print(f\"Initiliazing DocQAClient for OpenAI-like API. LLM='{llm_model_id}'\")\n",
    "qa_client = DocQAClient(\n",
    "    chroma_uri=LOCAL_CHROMA_URI,\n",
    "    openai_api_url=LOCAL_OPENAI_API_URI,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìö Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm.xplore.helpers import collect_files\n",
    "\n",
    "# Index some documents\n",
    "doc_files = collect_files(DOCS_PATH)\n",
    "print(f\"üìö Total files: {len(doc_files)}\")\n",
    "\n",
    "for doc_file in doc_files:\n",
    "    print(f\"üìñ Indexing doc '{doc_file.name}'\")\n",
    "    doc_data = json.loads(doc_file.open().read())\n",
    "    qa_client.index_document(\n",
    "        doc_pages=doc_data[\"text\"],\n",
    "        doc_meta={\n",
    "            \"id\": doc_data[\"sha1\"],\n",
    "            \"name\": doc_data[\"name\"],\n",
    "            \"type\": doc_data[\"type\"],\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß≤ Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'132d216ac71cd53e993251450243e588f548bf1d',\n",
       " '17f14f0a89df3ba96cc4966588682288e07fbc8d',\n",
       " '2a0ad74b29c929b9461f9bd2266518190fed4e27',\n",
       " '78944398eb5ef89fba1e4e7cd512066619982766',\n",
       " 'b2e85f84c2ea58571e5e270bf57f0ea98306ddb0',\n",
       " 'c90d52f64fb051f9d3687d03d3e6fa5e6bddfb5b',\n",
       " 'cf0fae563ffc10db0ecb2be19466b38afb2dc8cc',\n",
       " 'ebfc841fbb0e2126d6cc29df0aa73d0454711e69'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for indexed documents\n",
    "qa_client.list_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü¶ú Generate answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the DB and generate some answers\n",
    "from llm.xplore.rag.schemas import QuestionType\n",
    "\n",
    "\n",
    "qa_client.get_answers(\n",
    "    queries=[\"Who are the parties to this agreement?\"],\n",
    "    query_types=[QuestionType.EXTRACTIVE],\n",
    "    filters={\"id\": \"132d216ac71cd53e993251450243e588f548bf1d\"},\n",
    "    print_sources=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
