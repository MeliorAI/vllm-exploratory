{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔨 Setup\n",
    "\n",
    "⚠ In order to run this notebook you need two separate services:\n",
    "\n",
    "- ChromaDB: `chroma run --path ./data/chromadb --port 7000`\n",
    "- vLLM:     `vllm serve <your-model> --chat-template <your-template>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/code/LLM-exploratory\n",
      "/home/ubuntu/code/LLM-exploratory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/code/LLM-exploratory/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "LOCAL_OPENAI_API_URI = \"http://localhost:8000/v1\"\n",
    "LOCAL_CHROMA_URI = \"localhost:7000\"\n",
    "OPENAI_API_KEY = \"EMPTY\"\n",
    "DATA_PATH = \"./data\"\n",
    "DOCS_PATH = Path(DATA_PATH) / \"processed_docs\"\n",
    "\n",
    "DEFAULT_EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "DEFAULT_CHROMA_COLLECTION_NAME = \"llm.xplore\"\n",
    "\n",
    "# FIXME: VSCode has trouble importing local modules if the notebook is not at the root\n",
    "# See:\n",
    "# - https://github.com/microsoft/pylance-release/issues/3035\n",
    "# - https://github.com/microsoft/vscode-jupyter/issues/7926\n",
    "# -\n",
    "# So we change to the root of the repo (RUN ONLY ONCE!)\n",
    "%cd ..\n",
    "print(Path.cwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔌 OpenAI-like API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Check what model is available from the API\n",
    "client = OpenAI(api_key=OPENAI_API_KEY, base_url=LOCAL_OPENAI_API_URI)\n",
    "available_llms = client.models.list().data\n",
    "print(f\"Available models:\")\n",
    "print(\" - \" + \" - \".join(llm.id for llm in available_llms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/code/LLM-exploratory/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The best thing about LLMs is their ability to learn and improve over time\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai_like import OpenAILike\n",
    "\n",
    "llm = OpenAILike(\n",
    "    model=available_llms[0].id,\n",
    "    api_base=LOCAL_OPENAI_API_URI,\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "response = llm.complete(\"What is the best thing about LLMs?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓ DocQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🧬 Embeddings\n",
    "\n",
    "> NOTE: Try out `BGE` and `Instruct` embeddings VS `SBERT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/code/LLM-exploratory/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "[-0.039648737758398056, -0.0409272238612175, 0.020214181393384933, -0.00946382712572813, 0.00874999351799488]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "embeddings = embed_model.get_text_embedding(\"What is the best thing about LLMs?\")\n",
    "print(len(embeddings))\n",
    "print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 📚 Indexing\n",
    "\n",
    "> TODO: Check out [ingestion pipelines](https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Total files: 8\n",
      "📖 Indexing doc 'image_page.json'\n",
      "📖 Indexing doc 'california-commercial-lease-agreement_full_text.json'\n",
      "📖 Indexing doc 'DATA_README.json'\n",
      "📖 Indexing doc '20220405 - Glennmont Partners 2-way NDA.json'\n",
      "📖 Indexing doc 'ConstructionSampleClause2_image_only.json'\n",
      "📖 Indexing doc 'LinkedIn_MNG_4.19.2024_Adv, Mktg, Digial_Contract Cover Sheet_encryption_layer.json'\n",
      "📖 Indexing doc 'mixed_page.json'\n",
      "📖 Indexing doc 'Copy of MMG.DD NDA_Fails_OCR.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from llama_index.core.schema import Document\n",
    "from llm.xplore.helpers import collect_files\n",
    "\n",
    "# Index some documents\n",
    "doc_files = collect_files(DOCS_PATH)\n",
    "print(f\"📚 Total files: {len(doc_files)}\")\n",
    "\n",
    "documents = []\n",
    "for doc_file in doc_files:\n",
    "    print(f\"📖 Indexing doc '{doc_file.name}'\")\n",
    "    doc_data = json.loads(doc_file.open().read())\n",
    "    documents.append(Document(\n",
    "        doc_id=doc_data[\"sha1\"],\n",
    "        text=\"\\n\\n\".join(doc_data[\"text\"]),\n",
    "        extra_info={\n",
    "            \"name\": doc_data[\"name\"],\n",
    "            \"type\": doc_data[\"type\"],\n",
    "        },\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from IPython.display import Markdown, display\n",
    "import chromadb\n",
    "\n",
    "\n",
    "# db = chromadb.PersistentClient(path=str(DATA_PATH / \"chromadb\"))\n",
    "host, port = LOCAL_CHROMA_URI.split(\":\")\n",
    "db = chromadb.HttpClient(host, port)\n",
    "chroma_collection = db.get_or_create_collection(DEFAULT_CHROMA_COLLECTION_NAME)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "\n",
    "# TODO: Check index.insert for subsequent inserts\n",
    "# TODO: explore 'SemanticSplitterNodeParser'\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    "    transformations=[SentenceSplitter(chunk_size=100, chunk_overlap=20)],\n",
    ")\n",
    "\n",
    "# OR to get the index directly\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection [BEFORE]:  [Collection(id=e03d96d9-111b-48c9-8372-6fcf6847dfe7, name=llm.xplore)]\n",
      "Collections [AFTER]:  []\n"
     ]
    }
   ],
   "source": [
    "print(\"Collection [BEFORE]: \", db.list_collections())\n",
    "# db.delete_collection(DEFAULT_CHROMA_COLLECTION_NAME)\n",
    "# print(\"Collections [AFTER]: \", db.list_collections())\n",
    "\n",
    "# col = db.get_collection(DEFAULT_CHROMA_COLLECTION_NAME)\n",
    "# col.get()[\"metadatas\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🧲 Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering by metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: ebfc841fbb0e2126d6cc29df0aa73d0454711e69 [node ID: 2de34741-2e79-4d73-b131-39000d6e66a5]\n",
      "Score: None\n",
      "---\n",
      "Document ID: ebfc841fbb0e2126d6cc29df0aa73d0454711e69 [node ID: 95d33659-58af-493e-92a5-ca2ad76d44bd]\n",
      "Score: None\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.vector_stores import (\n",
    "    MetadataFilter,\n",
    "    MetadataFilters,\n",
    "    FilterOperator,\n",
    ")\n",
    "\n",
    "def print_sources(retrieved_nodes, print_text:bool = False):\n",
    "    for node in retrieved_nodes:\n",
    "        print(f\"Document ID: {node.node.ref_doc_id} [node ID: {node.node.node_id}]\")\n",
    "        if print_text:\n",
    "            print(f\"Text: {node.node.text}\")\n",
    "        print(f\"Score: {node.score}\")\n",
    "        print(\"---\")\n",
    "\n",
    "\n",
    "# filtering by metadata\n",
    "filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(\n",
    "            key=\"type\", operator=FilterOperator.EQ, value=\"Reseller Agreement\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Retrieve nodes for a specific metadata filter\n",
    "retriever = index.as_retriever(filters=filters)\n",
    "retrieved_nodes = retriever.retrieve(\"\")\n",
    "print_sources(retrieved_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering by doc IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 17f14f0a89df3ba96cc4966588682288e07fbc8d [node ID: e09a844f-f7e7-473d-b40d-7632f25768bd]\n",
      "Score: None\n",
      "---\n",
      "Document ID: 17f14f0a89df3ba96cc4966588682288e07fbc8d [node ID: 527df76b-8035-4e84-b67b-5a432a849040]\n",
      "Score: None\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Retrieve nodes for a specific document\n",
    "# FIXME: Filtering by `doc_ids` doesn't work.\n",
    "# See [llama_index/issues/14121](https://github.com/run-llama/llama_index/issues/14121)\n",
    "retriever = index.as_retriever(doc_ids=[\"ebfc841fbb0e2126d6cc29df0aa73d0454711e69\"])\n",
    "retrieved_nodes = retriever.retrieve(\"\")\n",
    "print_sources(retrieved_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering by Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 132d216ac71cd53e993251450243e588f548bf1d [node ID: 187cc315-3f02-48c7-8cb7-014b3251da41]\n",
      "Text: 24. Headings. \n",
      "The headings used in this Lease are for convenience of the parties only and shall not be \n",
      "considered in interpreting the meaning of any provision of this Lease. \n",
      "25. Successors. \n",
      "The provisions of this Lease shall extend to and be binding upon Landlord and Tenant and their \n",
      "respective legal representatives, successors and assigns. \n",
      "26. Consent.\n",
      "Score: 0.7072424642345998\n",
      "---\n",
      "Document ID: 132d216ac71cd53e993251450243e588f548bf1d [node ID: 0c758248-d190-43b9-8ab0-a49474ccbbcb]\n",
      "Text: Tenant \n",
      "shall be relieved from paying rent and other charges during any portion of the Lease term that the \n",
      "Leased Premises are inoperable or unfit for occupancy, or use, in whole or in part, for Tenant's \n",
      "Download Free Templates & Forms at Speedy Template http://www.SpeedyTemplate.com/\n",
      "\n",
      "\n",
      "purposes.\n",
      "Score: 0.7049398819743917\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Retrieve nodes for a specific query\n",
    "retriever = index.as_retriever()\n",
    "retrieved_nodes = retriever.retrieve(\"terms of the lease agreement in California\")\n",
    "print_sources(retrieved_nodes, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🦜 Generate answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " A Sales Navigator Advanced tool that allows our BDR and Sales teams to connect with"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query Data from the index\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    doc_ids=[\"78944398eb5ef89fba1e4e7cd512066619982766\"]\n",
    ")\n",
    "response = query_engine.query(\"What does LinkedIn provide?\")\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Glennmont Partners and the other party is not specified in the given context information."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"Who are the parties?\")\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "8.2 The Disclosing Party (including its Representatives) does not make any"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"Who are is the disclosing party?\")\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧪 Semantic chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1, breakpoint_percentile_threshold=95, embed_model=embed_model\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'print_table' from 'llm.xplore.helpers' (/home/ubuntu/code/LLM-exploratory/llm/xplore/helpers.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxplore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_table\n\u001b[1;32m      3\u001b[0m print_table(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSemantic Nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     columns\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     rows\u001b[38;5;241m=\u001b[39m[(node\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m], node\u001b[38;5;241m.\u001b[39mtext) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes],\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'print_table' from 'llm.xplore.helpers' (/home/ubuntu/code/LLM-exploratory/llm/xplore/helpers.py)"
     ]
    }
   ],
   "source": [
    "from llm.xplore.helpers import print_table\n",
    "\n",
    "print_table(\n",
    "    \"Semantic Nodes\",\n",
    "    columns=[\n",
    "        {\"name\": \"Doc Name\", \"style\": \"cyan\", \"max_width\": 25},\n",
    "        {\"name\": \"Node\", \"style\": \"magenta\", \"min_width\": 75},\n",
    "    ],\n",
    "    rows=[(node.metadata[\"name\"], node.text) for node in nodes],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
